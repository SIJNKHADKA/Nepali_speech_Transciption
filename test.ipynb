{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "906541b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading transcription metadata...\n",
      "Total audio files: 2219550\n",
      "Extracting MFCC & caching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2219550/2219550 [3:04:33<00:00, 200.43it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MFCC feature extraction complete!\n",
      "Saved MFCC cache to ROOT directory: mfcc_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.configs import SR\n",
    "from model.utils import clean_single_wav, gen_mfcc\n",
    "\n",
    "\n",
    "def load_text_metadata(texts_csv):\n",
    "    df = pd.read_csv(texts_csv)\n",
    "    return df[\"file\"].tolist(), df[\"text\"].tolist()\n",
    "\n",
    "\n",
    "def normalize_fname(f_name):\n",
    "    \"\"\"Ensure filename has NO extension\"\"\"\n",
    "    return os.path.splitext(f_name)[0]\n",
    "\n",
    "\n",
    "def generate_and_cache_mfcc(wavs_dir, texts_csv, cache_dir=\"mfcc_cache\"):\n",
    "\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    print(\"Loading transcription metadata...\")\n",
    "    file_ids, texts = load_text_metadata(texts_csv)\n",
    "\n",
    "    print(f\"Total audio files: {len(file_ids)}\")\n",
    "    print(\"Extracting MFCC & caching...\\n\")\n",
    "\n",
    "    for raw_name in tqdm(file_ids):\n",
    "\n",
    "        # remove extension if exists\n",
    "        f_name = normalize_fname(raw_name)\n",
    "\n",
    "        output_path = os.path.join(cache_dir, f\"{f_name}.npy\")\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            continue\n",
    "\n",
    "        flac_path = os.path.join(wavs_dir, f\"{f_name}.flac\")\n",
    "\n",
    "        if not os.path.exists(flac_path):\n",
    "            print(f\"Missing file: {flac_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            wav, _ = librosa.load(flac_path, sr=SR)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {flac_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        wav = clean_single_wav(wav)\n",
    "        mfcc = gen_mfcc(wav)\n",
    "        np.save(output_path, mfcc)\n",
    "\n",
    "    print(\"\\nMFCC feature extraction complete!\")\n",
    "    print(f\"Saved MFCC cache to ROOT directory: {cache_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_and_cache_mfcc(\n",
    "        wavs_dir=r\"E:\\DATASET\\nepali_merged\",\n",
    "        texts_csv=r\"E:\\DATASET\\metadata.csv\",\n",
    "        cache_dir=\"mfcc_cache\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3d802e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3344443682.py, line 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 72\u001b[1;36m\u001b[0m\n\u001b[1;33m    x, target, target_lengths, output_lengths =\u001b[0m\n\u001b[1;37m                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import edit_distance as ed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model.configs import SR, device_name, UNQ_CHARS, INPUT_DIM, NUM_UNQ_CHARS, MODEL_NAME\n",
    "from model.utils import batchify, indices_from_texts\n",
    "from model.model import get_model\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                LOAD CACHED MFCC DATA + TEXT LABELS\n",
    "# =====================================================================\n",
    "\n",
    "def load_cached_mfcc(cache_dir, texts_csv):\n",
    "    df = pd.read_csv(texts_csv)\n",
    "\n",
    "    mfcc_features = []\n",
    "    texts = []\n",
    "\n",
    "    print(\"Loading MFCC cache into RAM...\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        f = os.path.splitext(row[\"file\"])[0] + \".npy\"\n",
    "        mfcc_path = os.path.join(cache_dir, f)\n",
    "\n",
    "        if not os.path.exists(mfcc_path):\n",
    "            print(\"Missing MFCC:\", mfcc_path)\n",
    "            continue\n",
    "\n",
    "        mfcc = np.load(mfcc_path, allow_pickle=True)\n",
    "        mfcc_features.append(mfcc)\n",
    "        texts.append(row[\"text\"])\n",
    "\n",
    "    print(\"Finished loading MFCC cache.\")\n",
    "    return mfcc_features, texts\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                           TRAINING LOOP\n",
    "# =====================================================================\n",
    "\n",
    "def train_model(\n",
    "        model, optimizer,\n",
    "        train_feats, train_texts,\n",
    "        test_feats, test_texts,\n",
    "        epochs=40, batch_size=8\n",
    "    ):\n",
    "\n",
    "    history_train_loss = []\n",
    "    history_test_loss = []\n",
    "    history_test_cer = []\n",
    "\n",
    "    with tf.device(device_name):\n",
    "\n",
    "        for e in range(epochs):\n",
    "            print(f\"\\n========== EPOCH {e+1}/{epochs} ==========\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            # ---- Training ----\n",
    "            train_loss = 0\n",
    "            batches = 0\n",
    "\n",
    "            for start in tqdm(range(0, len(train_feats), batch_size), desc=\"Training\"):\n",
    "                end = min(start + batch_size, len(train_feats))\n",
    "\n",
    "                x, target, target_lengths, output_lengths =\n",
    "                    batchify(train_feats[start:end], train_texts[start:end], UNQ_CHARS)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output = model(x, training=True)\n",
    "                    loss = K.ctc_batch_cost(target, output, output_lengths, target_lengths)\n",
    "\n",
    "                grads = tape.gradient(loss, model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "                train_loss += np.mean(loss.numpy())\n",
    "                batches += 1\n",
    "\n",
    "            train_loss /= batches\n",
    "\n",
    "            # ---- Validation ----\n",
    "            test_loss = 0\n",
    "            test_cer = 0\n",
    "            batches = 0\n",
    "\n",
    "            for start in tqdm(range(0, len(test_feats), batch_size), desc=\"Validation\"):\n",
    "                end = min(start + batch_size, len(test_feats))\n",
    "\n",
    "                x, target, target_lengths, output_lengths =\n",
    "                    batchify(test_feats[start:end], test_texts[start:end], UNQ_CHARS)\n",
    "\n",
    "                output = model(x, training=False)\n",
    "\n",
    "                # Loss\n",
    "                loss = K.ctc_batch_cost(target, output, output_lengths, target_lengths)\n",
    "                test_loss += np.mean(loss.numpy())\n",
    "\n",
    "                # CER\n",
    "                input_len = np.ones(output.shape[0]) * output.shape[1]\n",
    "                decoded = K.ctc_decode(output, input_length=input_len,\n",
    "                                       greedy=False, beam_width=100)[0][0]\n",
    "\n",
    "                # Clean target indices\n",
    "                target_clean = [t[t != 0].tolist() for t in target]\n",
    "\n",
    "                predicted_clean = [\n",
    "                    p[p > 1].numpy().tolist() for p in decoded\n",
    "                ]\n",
    "\n",
    "                batch_cer = 0\n",
    "                for i in range(len(predicted_clean)):\n",
    "                    sm = ed.SequenceMatcher(predicted_clean[i], target_clean[i])\n",
    "                    d = sm.distance()\n",
    "                    batch_cer += d / len(target_clean[i])\n",
    "\n",
    "                test_cer += batch_cer / len(predicted_clean)\n",
    "                batches += 1\n",
    "\n",
    "            test_loss /= batches\n",
    "            test_cer /= batches\n",
    "\n",
    "            # Log\n",
    "            history_train_loss.append(train_loss)\n",
    "            history_test_loss.append(test_loss)\n",
    "            history_test_cer.append(test_cer)\n",
    "\n",
    "            print(f\"Epoch {e+1}:  Train Loss={train_loss:.4f} | \"\n",
    "                  f\"Val Loss={test_loss:.4f} | CER={test_cer*100:.2f}% | \"\n",
    "                  f\"time={time.time() - start_time:.2f}s\")\n",
    "\n",
    "    return history_train_loss, history_test_loss, history_test_cer\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                     PLOT LOSS & CER CURVES\n",
    "# =====================================================================\n",
    "\n",
    "def plot_training_curves(train_loss, test_loss, test_cer):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Loss Curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label=\"Train Loss\")\n",
    "    plt.plot(test_loss, label=\"Validation Loss\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"CTC Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # CER Curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(test_cer, label=\"Validation CER\")\n",
    "    plt.title(\"CER Curve (%)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"CER\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_curves.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#                           MAIN STARTS HERE\n",
    "# =====================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load cached MFCC & texts\n",
    "    feats, texts = load_cached_mfcc(\n",
    "        cache_dir=\"mfcc_cache\",\n",
    "        texts_csv=\"E:/DATASET/metadata.csv\"\n",
    "    )\n",
    "\n",
    "    # Train-test split\n",
    "    train_feats, test_feats, train_texts, test_texts = train_test_split(\n",
    "        feats, texts, test_size=0.2\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = get_model(\n",
    "        INPUT_DIM, NUM_UNQ_CHARS,\n",
    "        num_res_blocks=5,\n",
    "        num_cnn_layers=2,\n",
    "        cnn_filters=50,\n",
    "        cnn_kernel_size=15,\n",
    "        rnn_dim=170,\n",
    "        rnn_dropout=0.15,\n",
    "        num_rnn_layers=2,\n",
    "        num_dense_layers=1,\n",
    "        dense_dim=340,\n",
    "        model_name=MODEL_NAME,\n",
    "        rnn_type=\"lstm\",\n",
    "        use_birnn=True\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    # Train\n",
    "    tr_loss, val_loss, val_cer = train_model(\n",
    "        model, optimizer,\n",
    "        train_feats, train_texts,\n",
    "        test_feats, test_texts,\n",
    "        epochs=40,\n",
    "        batch_size=8\n",
    "    )\n",
    "\n",
    "    # Plot curves\n",
    "    plot_training_curves(tr_loss, val_loss, val_cer)\n",
    "\n",
    "    # Save final model\n",
    "    model.save(\"trained_model_from_mfcc_cache.h5\")\n",
    "    print(\"\\nModel Saved Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AJIT\\miniconda3\\envs\\asr_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py:37: UserWarning: A NumPy version >=1.23.5 and <2.5.0 is required for this version of SciPy (detected version 1.22.3)\n",
      "  from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n",
      "c:\\Users\\AJIT\\miniconda3\\envs\\asr_env\\lib\\site-packages\\llvmlite\\binding\\ffi.py:175: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ASR MODEL TRAINING WITH CACHED MFCC FEATURES\n",
      "============================================================\n",
      "\n",
      "Defining model architecture...\n",
      "âœ… Model defined successfully\n",
      "\n",
      "Loading metadata...\n",
      "Total samples in metadata: 2219550\n",
      "Loading cached MFCC features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 94374/2219550 [18:45<21:28:39, 27.49it/s] "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import edit_distance as ed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model.configs import SR, device_name, UNQ_CHARS, INPUT_DIM, MODEL_NAME, NUM_UNQ_CHARS\n",
    "from model.utils import batchify, indices_from_texts\n",
    "from model.model import get_model\n",
    "\n",
    "\n",
    "class TrainingHistory:\n",
    "    \"\"\"Store and visualize training metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_cers = []\n",
    "        self.epochs = []\n",
    "        self.train_times = []\n",
    "    \n",
    "    def update(self, epoch, train_loss, val_loss, val_cer, epoch_time):\n",
    "        self.epochs.append(epoch)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_cers.append(val_cer)\n",
    "        self.train_times.append(epoch_time)\n",
    "    \n",
    "    def plot_curves(self, save_path=\"training_curves.png\"):\n",
    "        \"\"\"Plot training and validation curves\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss curves\n",
    "        axes[0].plot(self.epochs, self.train_losses, 'b-', label='Train Loss', linewidth=2)\n",
    "        axes[0].plot(self.epochs, self.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[0].set_ylabel('CTC Loss', fontsize=12)\n",
    "        axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend(fontsize=10)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # CER curve\n",
    "        axes[1].plot(self.epochs, [cer * 100 for cer in self.val_cers], 'g-', linewidth=2)\n",
    "        axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[1].set_ylabel('Character Error Rate (%)', fontsize=12)\n",
    "        axes[1].set_title('Validation CER', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nðŸ“Š Training curves saved to: {save_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def save_metrics(self, save_path=\"training_metrics.csv\"):\n",
    "        \"\"\"Save metrics to CSV\"\"\"\n",
    "        df = pd.DataFrame({\n",
    "            'epoch': self.epochs,\n",
    "            'train_loss': self.train_losses,\n",
    "            'val_loss': self.val_losses,\n",
    "            'val_cer': self.val_cers,\n",
    "            'epoch_time_sec': self.train_times\n",
    "        })\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"ðŸ“ Training metrics saved to: {save_path}\")\n",
    "\n",
    "\n",
    "def load_cached_mfcc(file_ids, cache_dir=\"mfcc_cache\"):\n",
    "    \"\"\"Load pre-computed MFCC features from cache\"\"\"\n",
    "    mfccs = []\n",
    "    missing_files = []\n",
    "    \n",
    "    print(\"Loading cached MFCC features...\")\n",
    "    for f_name in tqdm(file_ids):\n",
    "        # Remove extension if present\n",
    "        f_name_clean = os.path.splitext(f_name)[0]\n",
    "        cache_path = os.path.join(cache_dir, f\"{f_name_clean}.npy\")\n",
    "        \n",
    "        if not os.path.exists(cache_path):\n",
    "            missing_files.append(f_name)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            mfcc = np.load(cache_path)\n",
    "            mfccs.append(mfcc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {cache_path}: {e}\")\n",
    "            missing_files.append(f_name)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"âš ï¸ Warning: {len(missing_files)} files missing from cache\")\n",
    "        print(f\"First few missing: {missing_files[:5]}\")\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(mfccs)} MFCC features from cache\\n\")\n",
    "    return mfccs\n",
    "\n",
    "\n",
    "def compute_cer(predicted_indices, target_indices):\n",
    "    \"\"\"Compute Character Error Rate for a batch\"\"\"\n",
    "    total_cer = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    for pred, truth in zip(predicted_indices, target_indices):\n",
    "        if len(truth) == 0:\n",
    "            continue\n",
    "        \n",
    "        sm = ed.SequenceMatcher(pred, truth)\n",
    "        ed_dist = sm.distance()\n",
    "        total_cer += ed_dist / len(truth)\n",
    "        valid_samples += 1\n",
    "    \n",
    "    return total_cer / valid_samples if valid_samples > 0 else 0\n",
    "\n",
    "\n",
    "def train_model_with_cache(model, optimizer, train_mfccs, train_texts, \n",
    "                           val_mfccs, val_texts, epochs=100, batch_size=50,\n",
    "                           save_dir=\"model_checkpoints\"):\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    history = TrainingHistory()\n",
    "    best_val_cer = float('inf')\n",
    "    \n",
    "    with tf.device(device_name):\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            len_train = len(train_mfccs)\n",
    "            len_val = len(val_mfccs)\n",
    "            \n",
    "            train_loss_total = 0\n",
    "            val_loss_total = 0\n",
    "            val_cer_total = 0\n",
    "            \n",
    "            train_batch_count = 0\n",
    "            val_batch_count = 0\n",
    "            \n",
    "            # ============= TRAINING PHASE =============\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Epoch {e+1}/{epochs} - TRAINING\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Shuffle training data\n",
    "            indices = np.random.permutation(len_train)\n",
    "            train_mfccs_shuffled = [train_mfccs[i] for i in indices]\n",
    "            train_texts_shuffled = [train_texts[i] for i in indices]\n",
    "            \n",
    "            for start in tqdm(range(0, len_train, batch_size), desc=\"Training\"):\n",
    "                end = min(start + batch_size, len_train)\n",
    "                \n",
    "                x, target, target_lengths, output_lengths = batchify(\n",
    "                    train_mfccs_shuffled[start:end], \n",
    "                    train_texts_shuffled[start:end], \n",
    "                    UNQ_CHARS\n",
    "                )\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    output = model(x, training=True)\n",
    "                    loss = K.ctc_batch_cost(target, output, output_lengths, target_lengths)\n",
    "                \n",
    "                grads = tape.gradient(loss, model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "                \n",
    "                train_loss_total += np.average(loss.numpy())\n",
    "                train_batch_count += 1\n",
    "            \n",
    "            # ============= VALIDATION PHASE =============\n",
    "            print(f\"\\nEpoch {e+1}/{epochs} - VALIDATION\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            for start in tqdm(range(0, len_val, batch_size), desc=\"Validation\"):\n",
    "                end = min(start + batch_size, len_val)\n",
    "                \n",
    "                x, target, target_lengths, output_lengths = batchify(\n",
    "                    val_mfccs[start:end], \n",
    "                    val_texts[start:end], \n",
    "                    UNQ_CHARS\n",
    "                )\n",
    "                \n",
    "                output = model(x, training=False)\n",
    "                loss = K.ctc_batch_cost(target, output, output_lengths, target_lengths)\n",
    "                \n",
    "                val_loss_total += np.average(loss.numpy())\n",
    "                val_batch_count += 1\n",
    "                \n",
    "                # Compute CER\n",
    "                input_len = np.ones(output.shape[0]) * output.shape[1]\n",
    "                decoded_indices = K.ctc_decode(\n",
    "                    output, \n",
    "                    input_length=input_len,\n",
    "                    greedy=False, \n",
    "                    beam_width=100\n",
    "                )[0][0]\n",
    "                \n",
    "                # Process target and predicted indices\n",
    "                target_indices = [sent[sent != 0].tolist() for sent in target]\n",
    "                predicted_indices = [sent[sent > 1].numpy().tolist() for sent in decoded_indices]\n",
    "                \n",
    "                batch_cer = compute_cer(predicted_indices, target_indices)\n",
    "                val_cer_total += batch_cer\n",
    "            \n",
    "            # ============= EPOCH SUMMARY =============\n",
    "            train_loss_avg = train_loss_total / train_batch_count\n",
    "            val_loss_avg = val_loss_total / val_batch_count\n",
    "            val_cer_avg = val_cer_total / val_batch_count\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            \n",
    "            # Update history\n",
    "            history.update(e + 1, train_loss_avg, val_loss_avg, val_cer_avg, epoch_time)\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"EPOCH {e+1} SUMMARY\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Train Loss:      {train_loss_avg:.4f}\")\n",
    "            print(f\"Val Loss:        {val_loss_avg:.4f}\")\n",
    "            print(f\"Val CER:         {val_cer_avg*100:.2f}%\")\n",
    "            print(f\"Epoch Time:      {epoch_time:.2f} sec\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_cer_avg < best_val_cer:\n",
    "                best_val_cer = val_cer_avg\n",
    "                best_model_path = os.path.join(save_dir, \"best_model.h5\")\n",
    "                model.save(best_model_path)\n",
    "                print(f\"ðŸŽ¯ New best model saved! CER: {best_val_cer*100:.2f}%\\n\")\n",
    "            \n",
    "            # Save checkpoint every 5 epochs\n",
    "            if (e + 1) % 5 == 0:\n",
    "                checkpoint_path = os.path.join(save_dir, f\"model_epoch_{e+1}.h5\")\n",
    "                model.save(checkpoint_path)\n",
    "                print(f\"ðŸ’¾ Checkpoint saved: {checkpoint_path}\\n\")\n",
    "        \n",
    "        # Save final model\n",
    "        final_model_path = os.path.join(save_dir, \"final_model.h5\")\n",
    "        model.save(final_model_path)\n",
    "        print(f\"\\nâœ… Final model saved: {final_model_path}\")\n",
    "        \n",
    "        # Plot and save curves\n",
    "        history.plot_curves(os.path.join(save_dir, \"training_curves.png\"))\n",
    "        history.save_metrics(os.path.join(save_dir, \"training_metrics.csv\"))\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TRAINING COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Best Validation CER: {best_val_cer*100:.2f}%\")\n",
    "        print(f\"Total Training Time: {sum(history.train_times)/3600:.2f} hours\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def load_data_from_cache(texts_csv, cache_dir=\"mfcc_cache\"):\n",
    "    \"\"\"Load texts and corresponding cached MFCC features\"\"\"\n",
    "    print(\"Loading metadata...\")\n",
    "    df = pd.read_csv(texts_csv)\n",
    "    file_ids = df[\"file\"].tolist()\n",
    "    texts = df[\"text\"].tolist()\n",
    "    \n",
    "    print(f\"Total samples in metadata: {len(file_ids)}\")\n",
    "    \n",
    "    # Load cached MFCCs\n",
    "    mfccs = load_cached_mfcc(file_ids, cache_dir)\n",
    "    \n",
    "    # Ensure texts and mfccs match\n",
    "    if len(mfccs) != len(texts):\n",
    "        print(f\"âš ï¸ Mismatch: {len(mfccs)} MFCCs vs {len(texts)} texts\")\n",
    "        # Align them (assuming order is preserved)\n",
    "        min_len = min(len(mfccs), len(texts))\n",
    "        mfccs = mfccs[:min_len]\n",
    "        texts = texts[:min_len]\n",
    "    \n",
    "    print(f\"âœ… Final dataset size: {len(mfccs)} samples\\n\")\n",
    "    return mfccs, texts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ASR MODEL TRAINING WITH CACHED MFCC FEATURES\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # ============= MODEL DEFINITION =============\n",
    "    print(\"Defining model architecture...\")\n",
    "    model = get_model(\n",
    "        INPUT_DIM, NUM_UNQ_CHARS, \n",
    "        num_res_blocks=5, \n",
    "        num_cnn_layers=2,\n",
    "        cnn_filters=50, \n",
    "        cnn_kernel_size=15, \n",
    "        rnn_dim=170, \n",
    "        rnn_dropout=0.15, \n",
    "        num_rnn_layers=2,\n",
    "        num_dense_layers=1, \n",
    "        dense_dim=340, \n",
    "        model_name=MODEL_NAME, \n",
    "        rnn_type=\"lstm\",\n",
    "        use_birnn=True\n",
    "    )\n",
    "    print(\"âœ… Model defined successfully\\n\")\n",
    "    \n",
    "    # ============= OPTIMIZER =============\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    # ============= LOAD DATA =============\n",
    "    mfccs, texts = load_data_from_cache(\n",
    "        texts_csv=r\"E:\\DATASET\\metadata.csv\",\n",
    "        cache_dir=\"mfcc_cache\"\n",
    "    )\n",
    "    \n",
    "    # ============= TRAIN-TEST SPLIT =============\n",
    "    print(\"Splitting data into train and validation sets...\")\n",
    "    train_mfccs, val_mfccs, train_texts, val_texts = train_test_split(\n",
    "        mfccs, texts, test_size=0.05, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples:   {len(train_mfccs)}\")\n",
    "    print(f\"Validation samples: {len(val_mfccs)}\\n\")\n",
    "    \n",
    "    # ============= TRAIN MODEL =============\n",
    "    history = train_model_with_cache(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_mfccs=train_mfccs,\n",
    "        train_texts=train_texts,\n",
    "        val_mfccs=val_mfccs,\n",
    "        val_texts=val_texts,\n",
    "        epochs=60,\n",
    "        batch_size=32,\n",
    "        save_dir=\"model_checkpoints\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Training pipeline completed successfully! ðŸŽ‰\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bc1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
